This application is an AI-assisted web service that lets users type natural-language dining requests—such as “Find me a romantic Thai place in NYC that stays open past 11 p.m. and serves a wide range of vegetarian foods with good cocktails”—into a chat-style interface and receive a short, ranked list of suitable restaurants, each accompanied by a plain-English explanation of how it meets (or fails to meet) the stated criteria. A single-page React front end (built with Vite and Tailwind for responsive styling) sends each user message to a FastAPI backend written in Python. That backend first calls a local large-language model served by Ollama (running a lightweight instruction-tuned model such as Mistral-7B or Llama-2-13B) OR an api call to a model running externally, not local, to extract structured filters—location, cuisine, ambiance, closing time, budget, dietary or cocktail requirements—from the free-form text. It then queries third-party data sources, beginning with the Yelp Fusion API and falling back to Google Places when Yelp returns too few matches, applying parameters like category, keyword, and open_at to respect the user’s constraints. The raw restaurant data are normalized and passed back to the same LLM, along with the original request, so it can analyze the list of optins, look at any and all restaurant data that is returned (tags, images, menu data, reviews etc), pick the 3 most closely matched options and draft concise, grounded explanations that cite matched attributes and flag any mismatches. Finally, the backend returns a markdown-formatted answer to the browser, which renders it as conversational messages. All services are containerized with Docker; during MVP the FastAPI server, Ollama daemon/external api, and a small Redis cache run on a single 8-GB RAM virtual machine (e.g., AWS t3.large) behind HTTPS (not a current priority), but the design is modular—AI calls are abstracted behind an AIClient interface so a cloud LLM can be swapped in later, and the search layer can incorporate additional providers or a self-hosted vector database for semantic retrieval. Observability comes from request and latency logs, rate limiting protects external API quotas, and the whole stack can scale horizontally by adding containers or moving the LLM to a dedicated host once traffic grows.
